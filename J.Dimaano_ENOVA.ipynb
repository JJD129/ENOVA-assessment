{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb55a256",
   "metadata": {},
   "source": [
    "# Objective\n",
    "\n",
    "You own a diamond business and you have to replenish your inventory. You have $\\$$5,000,000 budgeted towards the purchase of new diamonds. You have a historical list of diamonds that were purchased from various distributor vendors and the prices they retailed for. You also have a list of diamonds that are currently on the market, but this list does not include any set prices. Instead, you will place an offer for a diamond and the distributor will make a single yes-or-no decision to sell you the diamond at that price. If your offer is accepted then you receive the diamond and you will be awarded the difference between the retail price your company can eventually sell the diamond for and the price that you offered as profit. If you fail to make a purchase, you simply do not get anything for profit.\n",
    "What you will return to Enova is a copy of any code used, a description of your process, answers to the short answer questions, and the offers file with the offer column populated. The offer column should be populated only on diamonds you wish to make an offer on, and the sum of the offers needs to be less than or equal to $5,000,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e75e5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2175c63",
   "metadata": {},
   "source": [
    "# Diamond Buying\n",
    "\n",
    "Diamond grading standards use the 4C's:  \n",
    "\n",
    "* __Carat__ refers to the weight of the diamond. A unit measurement equals 1/5 gram and is subdivided into 100 points.\n",
    "* __Color__ is diamond tinge. Colorless diamonds are among the rarest. Other hues may be appear in diamonds. \n",
    "* __Clarity__ looks at the inclusions, the internal attributes, and blemishes, the external attributes. \n",
    "* __Cut__ affects the the diamond's overall radiance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf15c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "df = pd.read_csv('training.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182108a2",
   "metadata": {},
   "source": [
    "# Data Exploration/Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50196876",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3845e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646e0ed0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e24705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identiftying Carats max\n",
    "df[df['Carats']==11.92]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6f94f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# identiftying Price max\n",
    "df[df['Price']==1087785.00]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aac8d08",
   "metadata": {},
   "source": [
    "The training dataset shows there are 8,050 purchases. On average the diamond size that has been purchased is 1.27 carats, however there was a diamond that was purchased at one point that was 11.92 carats. There are only four vendors that historically the diamonds have been purchased from. Price and Retail features have an outlier. From the third quartile to the max value Price jumps from $\\$$14,660-$\\$$1,087,785 which would be a ~7000% increase. The retail price jump is just as high for the Retail price. \n",
    "\n",
    "However, the diamond that is 11.92 carats isn't providing the max price and retail price. The 11.92 carat diamond is a lesser quality diamong with a SI1 clarity, S color, and Fair Cut. Purcahsed from Vendor 2 the price paid was $\\$$154960 and sold at $\\$$231695.\n",
    "\n",
    "The diamond with the highest price and retail value is a 8.13 carat diamond with IF clarity, F color and unknown cut. This diamond ranks higher across other measures compared to the 11.92 carat diamond. This was purchased from Vendor 2 as well. \n",
    "\n",
    "Based on the table above, my initial assumptions show that Vendor 2 has sold a range of diamonds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb66b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histograms(df, columns, nrows=2, ncols=3, fig_width=15, fig_height=8):\n",
    "    fig, axs = plt.subplots(nrows, ncols, figsize=(fig_width, fig_height))\n",
    "\n",
    "    for i, column in enumerate(columns):\n",
    "        row = i // ncols\n",
    "        col = i % ncols\n",
    "        sns.histplot(data=df, x=column, element='step', stat='density', common_norm=False, kde=False, color='blue', ax=axs[row, col])\n",
    "        axs[row, col].set_title(column)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "plot_histograms(df, ['Carats', 'Price', 'Retail', 'LogPrice', 'LogRetail'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917be6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(dataframe, columns):\n",
    "    for col in columns:\n",
    "        Q1 = dataframe[col].quantile(0.25)\n",
    "        Q3 = dataframe[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - (1.5 * IQR)\n",
    "        upper_bound = Q3 + (1.5 * IQR)\n",
    "        dataframe = dataframe[(dataframe[col] > lower_bound) & (dataframe[col] < upper_bound)]\n",
    "    return dataframe\n",
    "\n",
    "# Remove outliers from Carats\n",
    "df = remove_outliers(df, df[['Carats', 'Price', 'Retail']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1625460c",
   "metadata": {},
   "source": [
    "LogPrice and LogRetail have been transformed to be normally distributed, however Carats, Price and Retail features are  right-skewed, meaning there are more lower priced and smaller sized diamonds that were purchased in the past. The outliers need to be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbac110c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Carats','Price','Retail']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb2e626",
   "metadata": {},
   "source": [
    "After removing the Carats, Price, and retail outliers, the data count dropped 1151 records from the dataset. Now the Carats feature is a bit more normally distributed. Price and Retail are still a little right-skewed but we can assume normality because it looks like historically half of the diamond purchases have been less than $\\$$5800. There might've been a few losses too because the minimum Price paid was $\\$$395 but the minimum Retail was $\\$$45. That's ~88% loss. It could be more if that isn't the same record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d21825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identiftying Retail min\n",
    "df[df['Retail']==45.00]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61393c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histograms(df, ['Carats', 'Price', 'Retail', 'LogPrice', 'LogRetail'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27338c2",
   "metadata": {},
   "source": [
    "My initial assumptions were correct. After removing the outliers you the majority of diamond sizes that were purchesd are <1 carat. Price in the first bin makes up the majority of offer prices, which would be roughly $\\$$1000 offers. Retail shows the second bin as the majority of what the diamonds are priced at roughly $\\$$2000-3000. This shows that the distribution of profits mostly following the Price, but there are occasional losses on diamond purchases as noted in the first bin of Retail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8af62b",
   "metadata": {},
   "source": [
    "For the model, I'll be remapping the 3C's Color, Cut, and Clarity to AGS color scale since none of the certifications are GIA:\n",
    "https://www.americangemsociety.org/understanding-diamond-color-the-4cs-of-diamonds/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afcb13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cert'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94159778",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cert'] = df['Cert'].replace({np.nan: 'noCert'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2629b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Clarity'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde1397c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Clarity'] = df['Clarity'].map({'SI1': 'Slightly Included',\n",
    "'SI2': 'Slightly Included',\n",
    "'VS2': 'Very Slightly Included', \n",
    "'VVS1': 'Very Very Slightly Included', \n",
    "'VS1': 'Very Slightly Included', \n",
    "'VVS2': 'Very Very Slightly Included', \n",
    "'I1': 'Included', \n",
    "'IF': 'Flawless/IF', \n",
    "'FL': 'Flawless/IF', \n",
    "'I2': 'Included',\n",
    "'I3': 'Included'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37bb04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Clarity'] = df['Clarity'].replace({np.nan: 'None'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c10bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Color'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246a334e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Color'] = df['Color'].map({'L':'Faint',\n",
    "'M':'Faint', \n",
    "'K':'Faint', \n",
    "'J':'Near Colorless', \n",
    "'F':'Colorless', \n",
    "'I':'Near Colorless', \n",
    "'H':'Near Colorless', \n",
    "'G':'Near Colorless', \n",
    "'E':'Colorless', \n",
    "'D':'Colorless', \n",
    "'N':'Very Light', \n",
    "'Ffcly':'Fancy Yellow',\n",
    "'Ffcdbrown':'Fancy Yellow', \n",
    "'Fvyellow':'Fancy Yellow', \n",
    "'Fdy':'Fancy Yellow', \n",
    "'Fyellow':'Fancy Yellow', \n",
    "'U':'Light', \n",
    "'Flyellow':'Fancy Yellow', \n",
    "'Ffg':'Fancy Yellow',\n",
    "'Q-r':'Very Light', \n",
    "'Lb':'Fancy Yellow', # doublecheck\n",
    "'Fiyellow':'Fancy Yellow', \n",
    "'Gy':'Fancy Yellow', # doublecheck\n",
    "'Fdpink':'Fancy Yellow', \n",
    "'Ffcy':'Fancy Yellow', \n",
    "'P':'Very Light', \n",
    "'Fiy':'Fancy Yellow', \n",
    "'Fly':'Fancy Yellow',\n",
    "'W':'Light', \n",
    "'O-p':'Very Light'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13221e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cut'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7faef65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cut'] = df['Cut'].replace({' ':'Uncut'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5397b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0fa1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X, y):\n",
    "    \"\"\"\n",
    "    Preprocesses the input data using a pipeline with SimpleImputer and StandardScaler.\n",
    "\n",
    "    Args:\n",
    "        X (pandas.DataFrame): Input data in a pandas DataFrame.\n",
    "        y (pandas.Series): Target variable in a pandas Series.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: Processed data in a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    # Create a pipeline with the imputer and scaler\n",
    "    preprocessor = make_pipeline(\n",
    "        SimpleImputer(strategy='mean'),\n",
    "        StandardScaler()\n",
    "    )\n",
    "\n",
    "    # Fit and transform the data using the pipeline\n",
    "    X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "    # Create a new DataFrame with the processed data\n",
    "    df_processed = pd.DataFrame(X_processed, columns=X.columns)\n",
    "\n",
    "    # Add the target variable to the new DataFrame\n",
    "    df_processed[y.name] = y\n",
    "\n",
    "    return df_processed\n",
    "X = df[['Depth', 'Table']]\n",
    "y = df['Price']\n",
    "df = preprocess_data(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76933fb",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d61a857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distribution of prices\n",
    "plt.figure(figsize = (15,10))\n",
    "sns.distplot(df['Price'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f910b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_4cs(df, y_col):\n",
    "    # Create a figure with 2 subplots side by side\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(20, 10))\n",
    "\n",
    "    # Check the relationship between carats and price\n",
    "    sns.scatterplot(x='Carats', y=y_col, data=df, ax=axs[0,0])\n",
    "    axs[0,0].set_title('Carats')\n",
    "\n",
    "    # Check the relationship between price and other features\n",
    "    sns.boxplot(x='Color', y=y_col, data=df, ax=axs[0,1])\n",
    "    axs[0,1].set_title('Color')\n",
    "\n",
    "    sns.boxplot(x='Clarity', y=y_col, data=df, ax=axs[1,0])\n",
    "    axs[1,0].set_title('Clarity')\n",
    "\n",
    "    sns.boxplot(x='Cut', y=y_col, data=df, ax=axs[1,1])\n",
    "    axs[1,1].set_title('Cut')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "plot_4cs(df, 'Price')\n",
    "plot_4cs(df, 'LogPrice')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb38332",
   "metadata": {},
   "source": [
    "Looking at the patterns between Price and the 4C's there isn't much deviance from the norm. Carats relative to Price shows a clear direct linear relationship. However, at each Carats there's a clear stacking, which I presume would be because of the other features that make up a quality diamond. What surprised me is that there isn't much deviation. I thought higher quality diamonds (Colorless, Flawless, Excellent) would have a higher spread. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88d1501",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.groupby(['Vendor']).agg(Price = ('Price', 'sum'))\n",
    "df1['Price']=df1['Price'].astype(int)\n",
    "plot = df1['Price'].plot.pie(autopct='%1.1f%%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da33f6d5",
   "metadata": {},
   "source": [
    "Vendor 2 has 43% of purchase history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98673d15",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0d45e8",
   "metadata": {},
   "source": [
    "### Do you think that any of the vendors are over or under charging for diamonds compared to the other vendors? Do you think they are selling the same kind of diamonds? How did you come to this conclusion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fddb405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which vendors are over or under charging:\n",
    "\n",
    "# Check the distribution of prices for each vendor\n",
    "sns.boxplot(x='Vendor', y='Price', data=df)\n",
    "plt.show()\n",
    "\n",
    "# Calculate the average price for each vendor\n",
    "vendor_prices = df.groupby(['Vendor','Color','Cut','Clarity'])['Price'].mean()\n",
    "vendor_prices_df = pd.DataFrame(vendor_prices)\n",
    "vendor_prices_df.reset_index(inplace=True)\n",
    "\n",
    "# Compare the vendor prices to the overall market prices\n",
    "market_mean = df.groupby(['Color','Cut','Clarity'])['Price'].mean()\n",
    "vendor_diff = vendor_prices - market_mean\n",
    "vendor_diff_df = pd.DataFrame(vendor_diff)\n",
    "vendor_diff_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dace383b",
   "metadata": {},
   "source": [
    "Vendor 2 has the widest spread, which confirms my inital assumption that Vendor 2 has sold a wide variety of diamonds since there are outliers. Vendor 2 and 3 more or less price their diamonds the same. Vendor 4 has the highest spread level, which may mean Vendor 4 has the higher prices. The mean price from Vendor 4 is the highest. Vendor 1 has the lowest spread, which might indicate they're the ones selling the lowest quality diamonds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5079d802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column for the market mean price\n",
    "df = df.merge(market_mean.reset_index().rename(columns={'Price': 'market_mean_price'}), \n",
    "              on=['Color','Cut','Clarity'], how='left')\n",
    "\n",
    "# Add a column for the vendor prices\n",
    "df = df.merge(vendor_prices.reset_index().rename(columns={'Price': 'vendor_price'}), \n",
    "              on=['Vendor','Color','Cut','Clarity'], how='left')\n",
    "\n",
    "# Calculate the difference between vendor prices and market mean prices\n",
    "df['difference'] = df['vendor_price'] - df['market_mean_price']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58a1fde",
   "metadata": {},
   "source": [
    "To assess if a vendor is over or undercharging a mean market price given a diamond's Cut, Color, and Clarity is determined. Then a vendor price given the diamond quality (Cut, Color, and Clarity) must be considered as well. Then the difference between vendor and market price will assess if a vendor is over or under charging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f4d834",
   "metadata": {},
   "outputs": [],
   "source": [
    "overcharge_df = df[df['difference'] > 0]\n",
    "grouped_vendor_df = overcharge_df.groupby('Vendor').sum().sort_values('difference', ascending=False)\n",
    "print(grouped_vendor_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7608512",
   "metadata": {},
   "source": [
    "__Vendor 4 is overcharging given the Color, Cut, and Clarity of the diamond__. Vendor 2 could most likely be overcharging as well given the spread and mean price is the same as Vendor 3, however given the amount of outliers Vendor 2 has it can't be determined at this point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932a0264",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3608a4",
   "metadata": {},
   "source": [
    "### What is the relationship between Carat and Retail Price? Why do you think the relationship takes this form?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4168d1",
   "metadata": {},
   "source": [
    "From the visuals above, the relationship between Carat and Retail price follow a clear direct linear relationship. meaning that as the carat weight of a diamond increases, the retail price will increase. The logic for this is because larger diamonds are not common and considered rare. Because of the infrequency a rare diamond comes on the market a diamond's value will increase leading to a higher price."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b8e2c1",
   "metadata": {},
   "source": [
    "# Model Build\n",
    "I'm going to use an ordinal encoding for color, clarity and cut and one hot encode the certification since it's not. ordinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c920d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cut'] = df['Cut'].replace({'Excellent':4, 'Very good':3, 'Good':2, 'Fair':1, 'Uncut':0})\n",
    "df['Color'] = df['Color'].replace({'Colorless':6, 'Near Colorless':5, 'Faint':4,'Very Light':3,'Light':2, 'Fancy Yellow':1})\n",
    "df['Clarity'] = df['Clarity'].replace({'Flawless/IF':5, 'Very Very Slightly Included':4, 'Very Slightly Included':3, 'Slightly Included':2, 'Included':1, 'None':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a902151",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns =['Cert', 'Known_Conflict_Diamond', 'Regions', 'Shape', 'Symmetry', 'Polish'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee9ac33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8ff9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['x', 'y', 'z']] = df['Measurements'].str.split('x', expand=True)\n",
    "df[['x', 'y', 'z']] = df[['x', 'y', 'z']].astype(float)\n",
    "df = df.drop(columns=['Measurements'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cdc0a2",
   "metadata": {},
   "source": [
    "The pearson correlation coefficient is calculated to get the general strength and direction of the relationship between the two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cfd0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.drop('id', axis=1).corr().round(2)\n",
    "plt.figure(figsize = (40,40))\n",
    "sns.heatmap(corr, annot = True, cmap = 'YlOrBr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c822741",
   "metadata": {},
   "source": [
    "Use all features that aren't highly correlated to Price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e5894b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# y = df['Price']\n",
    "# X = df.drop(['id','Price','Retail','LogPrice','LogRetail','market_mean_price','vendor_price','difference','x','y','z'], axis=1)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=312)\n",
    "# print(X_train.shape,y_train.shape)\n",
    "# print(X_test.shape,y_test.shape)\n",
    "\n",
    "# # Initialize the regression model\n",
    "# regression = LinearRegression()\n",
    "\n",
    "# # Fit the model on the PCA transformed data\n",
    "# regression.fit(X_train, y_train)\n",
    "\n",
    "# # Predict on new data\n",
    "# y_pred = regression.predict(X_test)\n",
    "\n",
    "\n",
    "# print(f'Intercept: {regression.intercept_:.3f}')\n",
    "\n",
    "# print('Coefficients:')\n",
    "# for name, coef in zip(X, regression.coef_):\n",
    "#     print(f' {name}: {coef}')\n",
    "\n",
    "# # Initialize k-fold cross-validator\n",
    "# kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# # Initialize lists to store the scores\n",
    "# mae_scores = []\n",
    "# mse_scores = []\n",
    "# r2_scores = []\n",
    "\n",
    "# # Perform k-fold cross-validation\n",
    "# for train_index, test_index in kf.split(X):\n",
    "#     X_cv_train, X_cv_test = X.iloc[train_index], X.iloc[test_index]\n",
    "#     y_cv_train, y_cv_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "#     # Fit the model on the training data\n",
    "#     regression.fit(X_cv_train, y_cv_train)\n",
    "\n",
    "#     # Predict on the test data\n",
    "#     y_pred = regression.predict(X_cv_test)\n",
    "\n",
    "#     # Calculate evaluation metrics\n",
    "#     mae = mean_absolute_error(y_cv_test, y_pred)\n",
    "#     mse = mean_squared_error(y_cv_test, y_pred)\n",
    "#     r2 = r2_score(y_cv_test, y_pred)\n",
    "\n",
    "#     # Append the scores to the lists\n",
    "#     mae_scores.append(mae)\n",
    "#     mse_scores.append(mse)\n",
    "#     r2_scores.append(r2)\n",
    "    \n",
    "# print('Mean Absolute Error:', np.mean(mae_scores))\n",
    "# print('Mean Squared Error:', np.mean(mse_scores))\n",
    "# print('R^2 Score:', np.mean(r2_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9c21d5",
   "metadata": {},
   "source": [
    "This model uses a linear regression model that is being trained on the data, with the goal of predicting the 'Price' variable based on low correlated features in the dataframe. The model is also using k-fold cross-validation (with 10 folds) to evaluate its performance, which is a technique to assess the generalization performance of a model by splitting the data into a training and testing set multiple times, where each time a different portion of the data is used as the testing set. The evaluation metrics that are reported are the mean absolute error (MAE), mean squared error (MSE), and R-squared score (R^2).\n",
    "\n",
    "* The Mean Absolute Error (MAE) is a measure of how close the predictions are to the true values. It is the average difference between the true and predicted values, and it is measured in the same units as the target variable. \n",
    "\n",
    "* The Mean Squared Error (MSE) is a measure of how close the predictions are to the true values. It is the average of the squared differences between the true and predicted values, and it is measured in the same units as the target variable. \n",
    "\n",
    "* The R^2 Score (R-squared score) is a measure of how much of the variability in the target variable is explained by the model. It ranges between 0 and 1, where 1 is a perfect fit and 0 is a poor fit. \n",
    "\n",
    "The R^2 score of 0.8595493996256692 suggests that the model has a good fit to the data, as it's close to 1. The MAE and MSE are also relatively low, indicating that the model's predictions are close to the true values.\n",
    "\n",
    "The MAE indicates that on average, the model's predictions are off by $\\$$1559.\n",
    "The MSE is 4521076, indicating that the model's predictions have a $\\$$4M spread.\n",
    "The R^2  score indicates that the model explains around 86% of the variance in the target variable (Retail)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba95741c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the features and target variable\n",
    "X = df.drop(['id', 'Price', 'Retail', 'LogPrice', 'LogRetail', 'market_mean_price', 'vendor_price', 'difference', 'x', 'y', 'z'], axis=1)\n",
    "y = df['Price']\n",
    "\n",
    "# Define the pipeline with preprocessing and model\n",
    "pipe = make_pipeline(\n",
    "    SimpleImputer(strategy='mean'),\n",
    "    StandardScaler(),\n",
    "    RandomForestRegressor(random_state=42)\n",
    ")\n",
    "\n",
    "# Define the grid of hyperparameters to search over\n",
    "param_grid = {\n",
    "    'randomforestregressor__n_estimators': [50, 100, 200],\n",
    "    'randomforestregressor__max_depth': [10, 20, 30, None],\n",
    "    'randomforestregressor__min_samples_split': [2, 5, 10],\n",
    "    'randomforestregressor__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Initialize the grid search with cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipe,\n",
    "    param_grid=param_grid,\n",
    "    cv=10,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the best hyperparameters and score\n",
    "print('Best hyperparameters:', grid_search.best_params_)\n",
    "print('Best score:', -grid_search.best_score_)\n",
    "\n",
    "# Extract the best model from the grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Perform k-fold cross-validation on the best model\n",
    "scores = []\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Fit the model on the training data\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test data\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Append the scores to the list\n",
    "    scores.append((mae, mse, r2))\n",
    "\n",
    "# Print the average scores across folds\n",
    "mae_scores, mse_scores, r2_scores = zip(*scores)\n",
    "print('Mean Absolute Error:', np.mean(mae_scores))\n",
    "print('Mean Squared Error:', np.mean(mse_scores))\n",
    "print('R^2 Score:', np.mean(r2_scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d10adb",
   "metadata": {},
   "source": [
    "PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9ab44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the features you want to include in your PCA analysis\n",
    "X = df.drop(['id','Price','Retail','LogPrice','LogRetail','market_mean_price','vendor_price','difference','x','y','z'], axis=1)\n",
    "\n",
    "# Initialize the PCA transformer\n",
    "pca = PCA(n_components=0.95)\n",
    "\n",
    "# Fit and transform the data\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Initialize the regression model\n",
    "regression = LinearRegression()\n",
    "\n",
    "# Fit the model on the PCA transformed data\n",
    "regression.fit(X_pca, df['Price'])\n",
    "\n",
    "# Predict on new data\n",
    "y_pred = regression.predict(X_pca)\n",
    "\n",
    "print(f'Intercept: {regression.intercept_:.3f}')\n",
    "\n",
    "print('Coefficients:')\n",
    "for name, coef in zip(X, regression.coef_):\n",
    "    print(f' {name}: {coef}')\n",
    "    \n",
    "# Initialize k-fold cross-validator\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store the scores\n",
    "mae_scores = []\n",
    "mse_scores = []\n",
    "r2_scores = []\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "for train_index, test_index in kf.split(X_pca):\n",
    "    X_train, X_test = X_pca[train_index], X_pca[test_index]\n",
    "    y_train, y_test = df['Price'][train_index], df['Price'][test_index]\n",
    "\n",
    "    # Fit the model on the training data\n",
    "    regression.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test data\n",
    "    y_pred = regression.predict(X_test)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Append the scores to the lists\n",
    "    mae_scores.append(mae)\n",
    "    mse_scores.append(mse)\n",
    "    r2_scores.append(r2)\n",
    "    \n",
    "print('Mean Absolute Error:', np.mean(mae_scores))\n",
    "print('Mean Squared Error:', np.mean(mse_scores))\n",
    "print('R^2 Score:', np.mean(r2_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650a99e5",
   "metadata": {},
   "source": [
    "This model uses a linear regression model that is being trained on the data, with the goal of predicting the 'Price' variable based on PCA selected features. PCA reduces dimensionality of the dataset while perserving as much variance. The model is also uses a k-fold cross-validation (with 10 folds) to evaluate its performance. The evaluation metrics that are reported are the mean absolute error (MAE), mean squared error (MSE), and R-squared score (R^2).\n",
    "\n",
    "The R^2 score of 0.8510699390654283 suggests that the model has a good fit to the data, as it's close to 1. The MAE and MSE are also relatively low, indicating that the model's predictions are close to the true values.\n",
    "\n",
    "The MAE indicates that on average, the model's predictions are off by $\\$$1596, $\\$$37 higher than the previous model\n",
    "The MSE is 4795140, indicating that the model's predictions have a $\\$$4M spread.\n",
    "The R^2  score indicates that the model explains around 85% of the variance in the target variable (Retail).\n",
    "\n",
    "Comparing the two models, both have similar performance, with the second model having a slightly higher MAE and MSE but also a slightly lower R^2 score. However, the difference is small, and both models are considered to have good performance. The first model is simpler and uses all the features of the data, but the second model uses PCA to reduce the dimensionality of the data and to find patterns in the data that are not easily visible in the original features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee1fe78",
   "metadata": {},
   "source": [
    "# Offers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378656d8",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6917d558",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_offers = pd.read_csv('offers.csv')\n",
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b285374",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_offers.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873c3e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram for carats and add to first subplot\n",
    "sns.histplot(data=df_offers, x='Carats', element='step', stat='density', common_norm=False, kde=False, color = 'blue')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a370599d",
   "metadata": {},
   "source": [
    "## PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858ace5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(dataframe, columns):\n",
    "    for col in columns:\n",
    "        Q1 = dataframe[col].quantile(0.25)\n",
    "        Q3 = dataframe[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - (1.5 * IQR)\n",
    "        upper_bound = Q3 + (1.5 * IQR)\n",
    "        dataframe = dataframe[(dataframe[col] > lower_bound) & (dataframe[col] < upper_bound)]\n",
    "    return dataframe\n",
    "df_offers = remove_outliers(df_offers, ['Carats'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9646f4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_offers['Cert'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189c9036",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_offers['Cert'] = df_offers['Cert'].replace({np.nan: 'noCert'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a55b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_offers['Clarity'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce02eed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_offers['Clarity'] = df_offers['Clarity'].map({'SI1': 'Slightly Included',\n",
    "'SI2': 'Slightly Included',\n",
    "'VS2': 'Very Slightly Included', \n",
    "'VVS1': 'Very Very Slightly Included', \n",
    "'VS1': 'Very Slightly Included', \n",
    "'VVS2': 'Very Very Slightly Included', \n",
    "'I1': 'Included', \n",
    "'IF': 'Flawless/IF', \n",
    "'FL': 'Flawless/IF', \n",
    "'I2': 'Included',\n",
    "'I3': 'Included'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f155b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_offers['Color'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644948bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_offers['Color'] = df_offers['Color'].map({'L':'Faint',\n",
    "'M':'Faint', \n",
    "'K':'Faint', \n",
    "'J':'Near Colorless', \n",
    "'F':'Colorless', \n",
    "'I':'Near Colorless', \n",
    "'H':'Near Colorless', \n",
    "'G':'Near Colorless', \n",
    "'E':'Colorless', \n",
    "'D':'Colorless', \n",
    "'N':'Very Light',\n",
    "'S-t':'Light', \n",
    "'T':'Light',\n",
    "'Ffancy darkbrown':'Fancy Yellow',\n",
    "'Flby':'Fancy Yellow'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a821cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_offers['Cut'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30ad6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_offers['Cut'] = df_offers['Cut'].replace({' ':'Uncut'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96edba9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_offers['Known_Conflict_Diamond'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2270d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_offers['Known_Conflict_Diamond'] = df_offers['Known_Conflict_Diamond'].replace({np.nan: 'Unknown'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd5af60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_offers.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0c8dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['Depth', 'Table']]\n",
    "y = df['Price']\n",
    "df = preprocess_data(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b36081",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_offers['Cut'] = df_offers['Cut'].replace({'Excellent':4, 'Very good':3, 'Good':2, 'Fair':1, 'Uncut':0})\n",
    "df_offers['Color'] = df_offers['Color'].replace({'Colorless':6, 'Near Colorless':5, 'Faint':4,'Very Light':3,'Light':2, 'Fancy Yellow':1})\n",
    "df_offers['Clarity'] = df_offers['Clarity'].replace({'Flawless/IF':5, 'Very Very Slightly Included':4, 'Very Slightly Included':3, 'Slightly Included':2, 'Included':1, 'None':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5244cf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_offers = pd.get_dummies(df_offers, columns =['Cert', 'Known_Conflict_Diamond', 'Regions', 'Shape', 'Symmetry', 'Polish'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568e0b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_offers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd4e16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_offers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad24745",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_offers.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d9e2db",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcf749e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the PCA transformer\n",
    "pca = PCA(n_components=15)\n",
    "\n",
    "# Select the features you want to include in your PCA analysis\n",
    "X = df_offers.drop(['id','Measurements','Offers'], axis=1)\n",
    "\n",
    "# Fit and transform the data\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Predict the offer price using the trained model\n",
    "offers_pred = regression.predict(X_pca)\n",
    "\n",
    "# Replace negative values with 0\n",
    "offers_pred = np.maximum(offers_pred, 0)\n",
    "\n",
    "# Add the predicted offer prices to the offers dataset\n",
    "df_offers['Offers'] = offers_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8378f354",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_offers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee70964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the total sum of offers is less than or equal to $5,000,000\n",
    "if df_offers['Offers'].sum() > 5000000:\n",
    "    raise ValueError('Total sum of offers exceeds budget')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867fc510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the relationship between carats and price\n",
    "sns.scatterplot(x='Carats', y='Offers', data=df_offers)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f442efab",
   "metadata": {},
   "source": [
    "Since we got the ValueError above, another step must be taken to determine which diamonds to make an offer on. First Offers will be sorted with top offers first. The script will then go through each row to check the current offer plus the total susum is over the $\\$$5M budget. If it is then the row offer is replaced with a 0, removing the offer on the diamond. This script will continue through the rows until the sum of offers hit under budget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cb2a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the diamonds by the predicted offer price in descending order\n",
    "df_offers = df_offers.sort_values(by='Offers', ascending=False)\n",
    "\n",
    "# Initialize a variable to track the total sum of offers\n",
    "total_offers = 0\n",
    "\n",
    "# Iterate through the sorted dataset\n",
    "for i, row in df_offers.iterrows():\n",
    "    # Check if the current offer price plus the total sum of offers exceeds the budget\n",
    "    if total_offers + row['Offers'] > 5000000:\n",
    "        # If it does, remove the offer from this diamond\n",
    "        df_offers.at[i, 'Offers'] = 0\n",
    "    # Update the total sum of offers\n",
    "    total_offers += df_offers.at[i, 'Offers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4899acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the final dataset\n",
    "df_offers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3d85ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the total sum of offers is less than or equal to $5,000,000\n",
    "if df_offers['Offers'].sum() > 5000000:\n",
    "    raise ValueError('Total sum of offers exceeds budget')\n",
    "else:\n",
    "    print('We are within budget')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bef5602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting only ID and offers feature to join onto original dataframe\n",
    "df_offers = df_offers.loc[:, ['id', 'Offers']]\n",
    "df_offers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494c5d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_offers_FINAL = pd.read_csv('offers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5180ee6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the two dataframes on the 'id' column\n",
    "df_offers_FINAL.drop(columns=['Offers'], inplace=True)\n",
    "df_offers_FINAL = df_offers_FINAL.merge(df_offers, on='id', how='left')\n",
    "df_offers_FINAL.to_csv('offers.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1309841f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
